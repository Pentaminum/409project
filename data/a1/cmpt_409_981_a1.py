# -*- coding: utf-8 -*-
"""CMPT-409_981 A1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11AFh2z8PYZVC8jDyOr2nvZPE2j9ZehlQ
"""

##Question5 - (1)
import numpy as np
from numpy.linalg import eig
from numpy.linalg import norm
import matplotlib.pyplot as plt

X = np.loadtxt("/content/X.csv", delimiter=",")
y = np.loadtxt("/content/y.csv", delimiter=",")

n,d = X.shape

print(X.shape)
print(y.shape)

"""# New Section"""

#computing Hessian of f(w)
H = np.dot(np.transpose(X),X) + 1/n * np.eye(d)

#computing eigenvalues
eigenvals,_ = np.linalg.eig(H)

#L is the maximum eigenvalue of the hessian
L = np.max(eigenvals)
print(L)

#compute gradient
def comp_grad(X, w, y, n):
  grad = np.dot(np.transpose(X), (np.dot(X, w) - y)) + 1/n * w
  return grad

#GD implementation with eta = 1/L
constant_grad_norm = []
constant_grad_evals = []
constant_func_evals = []
max_iterations = 5000
def constant_eta():
  w = np.zeros(d)
  eta = 1/L

  for i in range(max_iterations):
    constant_func_evals.append(comp_f(X,y,w,n))
    #computing grad of f(w)
    gradient = comp_grad(X, w, y, n)

    constant_grad_evals.append(1)

    #GD update
    w = w - eta * gradient

    constant_grad_norm.append(np.linalg.norm(gradient))
constant_eta()

plt.yscale('log')
plt.plot(constant_grad_norm)

##Question5 - (2)
els_grad_norm = []
els_grad_evals = []
els_func_evals = []

max_iterations = 1000
def exact_line_search():
  w = np.zeros(d)

  for i in range(max_iterations):
    els_func_evals.append(comp_f(X,y,w,n))
    grad = comp_grad(X, w, y, n)
    els_grad_evals.append(1)
    eta_num = np.linalg.norm(grad)**2
    H = np.dot(np.transpose(X),X) + 1/n * np.eye(d)
    temp = np.dot(H, grad)
    eta_denom = np.dot(grad.T, temp)

    eta = eta_num/eta_denom
    w = w - eta * grad

    els_grad_norm.append(np.linalg.norm(grad))
exact_line_search()

plt.yscale('log')
plt.plot(els_grad_norm)

def comp_f(X, y, w, n):
  # print(X.shape)
  res = 0.5 * (((np.linalg.norm(np.dot(X, w) - y))**2) + 1/n * (np.linalg.norm(w)**2))
  return res

#Question 5 - (3)
max_iterations = 500
eta_max = 10
c = 0.5
b = 0.9

max_backtracking_iters = 5000

armijo_ls_grad_norm = []
armijo_ls_func = []
armijo_ls_grad_evals = []
armijo_ls_func_evals = []
#Implementing Armijo Line search algorithm
def armijo_line_search():

  w = np.zeros(d)
  for i in range(max_iterations):
    eta_k = eta_max

    grad = comp_grad(X, w, y, n)

    armijo_ls_grad_evals.append(1)

    if np.linalg.norm(grad) <= 1e-16:
      break

    func_val = comp_f(X, y, w, n)
    armijo_ls_func_evals.append(func_val)

    backtracking_done = 0
    for j in range(max_backtracking_iters):
      armijo_ls_func_evals.append(func_val)
      if comp_f(X, y, w - eta_k * grad, n) <=  func_val - c * eta_k * np.linalg.norm(grad)**2:
        backtracking_done = 1
        break
      else:
        eta_k = eta_k * b
    eta = eta_k

    w = w - eta * grad

    armijo_ls_grad_norm.append(np.linalg.norm(grad))

  return w

armijo_line_search()
plt.yscale('log')
plt.plot(armijo_ls_grad_norm)

"""#Question 5 - (4)

Grad norms vs oracle calls
"""

plt.figure()
plt.yscale('log')
plt.xlabel('oracle calls')
plt.ylabel('Gradient norms')
plt.plot(np.arange(len(constant_grad_evals)), constant_grad_norm, label ='constant grad')
plt.plot(np.arange(len(els_grad_evals)), els_grad_norm, label ='els grad')
plt.plot(np.arange(len(armijo_ls_grad_evals)), armijo_ls_grad_norm, label ='armijo')
plt.legend()
plt.title('gradient norms vs oracle calls')

"""Func vs oracle calls"""

plt.figure()
plt.yscale('log')
plt.xlabel('oracle calls')
plt.ylabel('function values')
plt.plot(np.arange(len(constant_func_evals)), constant_func_evals, label ='constant grad')
plt.plot(np.arange(len(els_func_evals)), els_func_evals, label ='els grad')
plt.plot(np.arange(len(armijo_ls_func_evals)), armijo_ls_func_evals, label ='armijo')
plt.legend()
plt.title('Function values vs oracle calls')