# -*- coding: utf-8 -*-
"""Q3_Q4_A2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P4YImk3jLlML3ht_okYn3JP94GzQWmjH
"""

##imports
import numpy as np
from numpy.linalg import eig, norm
import matplotlib.pyplot as plt
import time

X = np.loadtxt("/content/X.csv", delimiter=",")
y = np.loadtxt("/content/y.csv", delimiter=",")

n,d = X.shape

print(X.shape)
print(y.shape)

epsilon = 1e-4
XtX = np.dot(np.transpose(X), X)
Xty = np.dot(np.transpose(X), y)
w0 = np.ones(d)
T = 1000

#compute gradient
def comp_grad(w):
  grad = XtX @ w - Xty + (1/n)*w
  return grad

#computing Hessian of f(w)
H = np.dot(np.transpose(X),X) + 1/n * np.eye(d)

#computing eigenvalues
eigenvals,_ = np.linalg.eig(H)

#L is the maximum eigenvalue of the hessian
L = np.max(eigenvals)
m = np.min(eigenvals)

"""Conjugate Gradient Implementation"""

def CG(A, b, w):
  r = b - A @ w
  p = r
  grad_norms = []
  for k in range(T):
    grad_norms.append(np.linalg.norm(r))
    if np.linalg.norm(r) < epsilon:
      break
    alpha = np.linalg.norm(r) ** 2/(p.T@A@p)
    wk = w + alpha*p
    rk = r - alpha*A@p
    gamma = np.linalg.norm(rk) ** 2/np.linalg.norm(r) ** 2
    p = rk + gamma*p
    r = rk
    w = wk

  return (grad_norms,w)

"""gradient
descent update with the step-size set according to the exact line-search when minimizing the
function f (w) := 1
2 wTAw − bw
"""

cg_grad_norms = CG(XtX + np.eye(d) * (1 / n), Xty, w0)
print(cg_grad_norms[0])
plt.yscale('log')
plt.plot(cg_grad_norms[0])

"""Implementation Heavy ball momentum"""

def HB(w):
  eta = 4 / np.square(np.sqrt(L) + np.sqrt(m))
  beta = np.square((np.sqrt(L / m) - 1) / (np.sqrt(L / m) + 1))
  wk = w
  wk_prev = 0
  grad_norms = []

  for k in range(T):
      grad_norms.append(np.linalg.norm(comp_grad(wk)))
      if np.linalg.norm(np.linalg.norm(comp_grad(wk))) < epsilon:
          break
      wkk = wk - eta * comp_grad(wk) + beta * (wk - wk_prev)
      wk_prev = wk
      wk = wkk
  return grad_norms

hb_grad_norms = HB(w0)
print(hb_grad_norms)
plt.yscale('log')
plt.plot(hb_grad_norms)

"""## Implementation of Nesterov acceleration with the theoretical hyper-parameters we derived for the convex case"""

def NA(w):
  wk = w
  wk_prev = 0
  eta = 1 / L
  grad_norms = []
  lambda_list = []
  for k in range(T):
    l = (1 + np.sqrt(1 + 4 * lambda_list[-1] ** 2)) / 2 if k > 0 else 0
    lambda_list.append(l)

  beta_list = [0] * T
  for k in range(T-1):
      beta_list[k+1] = (lambda_list[k] - 1) / lambda_list[k+1]

  for k in range(T):
    grad_norms.append(np.linalg.norm(comp_grad(wk)))
    if np.linalg.norm(np.linalg.norm(comp_grad(wk))) < epsilon:
        break
    vk = wk + beta_list[k] * (wk - wk_prev)
    wkk = vk - eta * comp_grad(vk)
    wk_prev = wk
    wk = wkk

  return grad_norms

na_grad_norms = NA(w0)
print(na_grad_norms)
plt.yscale('log')
plt.plot(na_grad_norms)

"""# **Question 4**

### i) Implementation of Newton Method with c = 1/2 and ηmax = 1 and β = 0.9

Utility functions
"""

y_X = - np.diag(y) @ X
def logistic_exp(w):
    return np.exp(-y * np.dot(X, w))

def logictic(w):
    g = np.log1p(np.exp(y_X @ w)).mean(axis=0)
    return g + (1/ (2 * n)) * np.linalg.norm(w)

def logistic_grad(w):
    e = np.exp(y_X @ w)
    g = e / (1 + e)
    g = np.expand_dims(g, 1) * y_X
    g = np.sum(g, axis=0)
    reg = (1 / n) * w
    return g / n + reg

def logistic_hessian(w):
    e = np.exp(y_X @ w)
    g = e / (1 + e)
    h = g - e ** 2 / ((1 / (1 + e)) ** (-2))
    return (X.T @ np.diag(h) @ X) / n + (1/n)

def newton(w):
  times = []
  grad_norms = []
  for k in range(50):
    g = logistic_grad(w)
    gNorm = np.linalg.norm(g)
    grad_norms.append(gNorm)
    times.append(time.time())
    if gNorm < epsilon:
      break
    eta = 1
    hessian = logistic_hessian(w)
    dk = CG(hessian, g, np.ones(d))[1]
    while logictic(w - eta * dk) > logictic(w) - 0.5 * eta * np.dot(g,dk):
      eta = eta * 0.9

    w = w - eta * dk

  return (grad_norms, times)

newton_grad_norms = newton(w0)
print(newton_grad_norms)
plt.yscale('log')
plt.plot(newton_grad_norms[0])

"""### ii) Implementation of Gradient Descent with the Backtracking Armijo line-search with c = 1/2 and ηmax = 1"""

def backtracking_als(w):
  grad_norms = []
  times = []
  for k in range(50):
    times.append(time.time())
    gradient = logistic_grad(w)
    grad_norm = np.linalg.norm(gradient)
    grad_norms.append(grad_norm)
    if grad_norm < epsilon:
      break
    eta = 1
    l = logictic(w)
    c_g = 0.5 * grad_norm ** 2
    while logictic(w - eta * gradient) > l - eta * c_g:
      eta = eta * 0.9
    w = w - eta * gradient

  return (grad_norms, times)

backtracking_als_grad_norms = backtracking_als(w0)
print(backtracking_als_grad_norms)
plt.yscale('log')
plt.plot(backtracking_als_grad_norms[0])

"""Newton vs GD - number of iteration"""

print(backtracking_als_grad_norms[0])
print(newton_grad_norms[0])
plt.yscale('log')
plt.plot(backtracking_als_grad_norms[0], label='GD')
plt.plot(newton_grad_norms[0], label='newton')
plt.title('Newton vs GD - number of iteration')

"""Newton vs GD - Wall clock time"""

print(backtracking_als_grad_norms[1])
print(newton_grad_norms[1])
plt.yscale('log')
plt.plot(backtracking_als_grad_norms[1], label='GD')
plt.plot(newton_grad_norms[1], label='newton')
plt.title('Newton vs GD - Wall clock time')